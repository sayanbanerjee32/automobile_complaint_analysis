---
title: "Automobile complaint Analysis"
author: "Sayan Banerjee"
date: "August 2, 2017"
output: 
  html_document: 
    number_sections: yes
    toc: yes
---
#Synopsis

This is analysis of complaints for an automobile company. This includes topic analysis and sentiment analysis.  
  
Loading the required packages for the data analysis  

```{r loadpackages, echo=TRUE, results="hide", message=FALSE,warning=FALSE}

library(ggplot2)

library(syuzhet)
library(NLP) 
library(tm)
library(slam)
library(openNLP)
#library(quanteda)
library(lexRankr)
library(LSAfun)

library(RWeka)
library(cluster)
library(dendextend)

library(wordcloud)

library(topicmodels)
library(Rmpfr)

library(dplyr)
library(lazyeval)
library(stringi)
library(stringr)
library(corrplot)

library(shiny)
library(LDAvis)
library(servr)

library(igraph)
library(visNetwork)
library(RColorBrewer)

library(knitr)
library(xtable)

```



```{r setopions, echo=FALSE}
opts_chunk$set(echo=TRUE, results="asis",cache=TRUE,tidy=FALSE,warning=FALSE)
opts_chunk$set(out.width='750px', dpi=200)
```


# User defined function

All user defined functions are currently hidden  

```{r define_functions, echo=FALSE}

strtable <- function(df, n=4, width=60, 
                     n.levels=n, width.levels=width, 
                     factor.values=as.character) {
  stopifnot(is.data.frame(df))
  tab <- data.frame(variable=names(df),
                    class=rep(as.character(NA), ncol(df)),
                    levels=rep(as.character(NA), ncol(df)),
                    examples=rep(as.character(NA), ncol(df)),
                    stringsAsFactors=FALSE)
  collapse.values <- function(col, n, width) {
    result <- NA
    for(j in 1:min(n, length(col))) {
      el <- ifelse(is.numeric(col),
                   paste0(col[1:j], collapse=', '),
                   paste0('"', col[1:j], '"', collapse=', '))
      if(nchar(el) <= width) {
        result <- el
      } else {
        break
      }
    }
    if(length(col) > n) {
      return(paste0(result, ', ...'))
    } else {
      return(result)
    }
  }
  
  for(i in seq_along(df)) {
    if(is.factor(df[,i])) {
      tab[i,]$class <- paste0('Factor w/ ', nlevels(df[,i]), ' levels')
      tab[i,]$levels <- collapse.values(levels(df[,i]), n=n.levels, width=width.levels)
      tab[i,]$examples <- collapse.values(factor.values(df[,i]), n=n, width=width)
    } else {
      tab[i,]$class <- class(df[,i])[1]
      tab[i,]$examples <- collapse.values(df[,i], n=n, width=width)
    }
    
  }
  
  class(tab) <- c('strtable', 'data.frame')
  return(tab)
}

#' Prints the results of \code{\link{strtable}}.
#' @param x result of code \code{\link{strtable}}.
#' @param ... other parameters passed to \code{\link{print.data.frame}}.
#' @export
print.strtable <- function(x, ...) {
  NextMethod(x, row.names=FALSE, ...)
}

tagPOS <-  function(x, ...) {
  s <- as.String(x)
  if(length(s) > 0 & nchar(s) > 1)
  {
    word_token_annotator <- Maxent_Word_Token_Annotator()
    a2 <- Annotation(1L, "sentence", 1L, nchar(s))
    a2 <- annotate(s, word_token_annotator, a2)
    a3 <- NLP::annotate(s, Maxent_POS_Tag_Annotator(), a2)
    a3w <- a3[a3$type == "word"]
    POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
    POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
    pos_ret <- list(POStagged = POStagged, POStags = POStags)
  } else {
    pos_ret <- list(POStagged = s, POStags = "")
  }
  pos_ret
    
}
annotators <- list(sent_token = Maxent_Sent_Token_Annotator(),
                   word_token = Maxent_Word_Token_Annotator(),
                   pos_tag    = Maxent_POS_Tag_Annotator())
extractPOS <- function(x, POSregex_list, 
                       POSregex_exclude_list = "",
                       with_tag = FALSE,
                       ann = annotators) {
  #print(paste("extractPOS started: ",Sys.time()))
  x <- as.String(x)
  #print(x)
  if(length(x) > 0 & nchar(x) > 1)
  {
      wordAnnotation <- NLP::annotate(x, list(ann$sent_token, ann$word_token))
      #print(paste("Maxent_Sent_Token_Annotator and Maxent_Word_Token_Annotator completed: ",
      #Sys.time()))
      POSAnnotation <- annotate(x, ann$pos_tag, wordAnnotation)
      #print(paste("Maxent_POS_Tag_Annotator completed: ",Sys.time()))
      POSwords <- subset(POSAnnotation, type == "word")
      tags <- sapply(POSwords$features, '[[', "POS")
      if(length(POSregex_list) > 0 & nchar(POSregex_list[1]) > 1)
      {
        thisPOSindex <- grep(paste(POSregex_list,collapse="|"), tags)
        if (length(thisPOSindex) > 0)
        {
          if(length(POSregex_exclude_list) > 0 & 
             nchar(POSregex_exclude_list[1]) > 1)
          {
            thisPOSindex <- setdiff(thisPOSindex, 
            grep(paste(POSregex_exclude_list,collapse="|"), tags))
          }
          if(with_tag)
          {
            tokenizedAndTagged <- sprintf("%s/%s", 
                                          x[POSwords][thisPOSindex],
                                          tags[thisPOSindex])
          } else tokenizedAndTagged <- as.character(x[POSwords])[thisPOSindex]
        } else tokenizedAndTagged <- ""
    
      } else {
        tokenizedAndTagged <- sprintf("%s/%s", x[POSwords], tags)
      }
      untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
  } else {
    untokenizedAndTagged <- x
  }
  
  untokenizedAndTagged
}

dtm.generate <- function(df,ng,
                         sparse=1,
                         spl_sym = "",
                         my_stop_word_file="",
                         keep.id=FALSE,
                         remove_non_english_char = TRUE,
                         removeNum = TRUE,
                         removePunc = TRUE,
                         removeStpWords = TRUE,
                         doStem = TRUE,
                         doIDF = TRUE,
                         doNormTf = TRUE)
{
  
  if (remove_non_english_char)
  {
    for(c in 1:ncol(df))
    {
    
      df[,colnames(df)[c]] <- gsub("[^\x20-\x7E]", "",
                                             df[,colnames(df)[c]])
    }
  }
  if(keep.id == TRUE)
  {
    # tutorial on rweka - http://tm.r-forge.r-project.org/faq.html
     m <- list(id = "ID", content = colnames(df)[2])
     myReader <- readTabular(mapping = m)

     corpus <- VCorpus(DataframeSource(df), readerControl = list(reader = myReader))
   
    # Manually keep ID information from http://stackoverflow.com/a/14852502/1036500
#       for (i in 1:length(corpus)) {
#         attr(corpus[[i]], "id") <- df$ID[i]
#         #corpus[[i]]$meta$ID <- df$ID[i]
#       }
  } else
  {
    corpus <- Corpus(VectorSource(df[,1])) # create corpus for TM processing
  }
  
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stripWhitespace)
  if (removeStpWords) corpus <- tm_map(corpus, removeWords, 
                                       c(stopwords("SMART"),stopwords("english")))

  if (removeNum) corpus <- tm_map(corpus, removeNumbers) 
  if(length(spl_sym) > 0 & sum(nchar(spl_sym)) > 0)
  {
    corpus <- tm_map(corpus, content_transformer(gsub), 
                     pattern = paste(spl_sym,collapse="|"), replacement = " ", fixed=TRUE)
  }
  if (removePunc) corpus <- tm_map(corpus, removePunctuation)
   if (doStem) corpus <- tm_map(corpus, stemDocument)
  if(my_stop_word_file !="")
  {
    content_specific_stop_words <- read.csv(my_stop_word_file,
                                            header=F,stringsAsFactors = F)
    corpus <- tm_map(corpus, removeWords, content_specific_stop_words[,1])
  }
  #corpus <- tm_map(corpus, PlainTextDocument)
  if (ng >1)
  {
    options(mc.cores=1) # http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka/20251039#20251039
    # this stopped working in new server environment
    #BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = ng, max = ng)) # create n-grams
    nGramTokenizer <-
      function(x)
        unlist(lapply(ngrams(words(x), ng), paste, collapse = " "), use.names = FALSE)
    if(doIDF)
    {
        dtm <- DocumentTermMatrix(corpus, control = list(tokenize = nGramTokenizer,
                                                   weighting = function(x)                                                          
                                                          weightTfIdf(x, normalize = doNormTf))) # create tdm from n-grams
    } else
    {
          dtm <- DocumentTermMatrix(corpus, control = list(tokenize = nGramTokenizer,
                                                   weighting = weightTf)) # create tdm from n-grams
    }
    
  }
  else
  {
    if(doIDF)
    {
        dtm <- DocumentTermMatrix(corpus,control = list(weighting = function(x)                                                          
                                                            weightTfIdf(x, normalize = doNormTf)))
    } else
    {
      dtm <- DocumentTermMatrix(corpus,control = list(weighting = weightTf))
    }
    
  }
  if(sparse != 1)
  {
    dtms <- removeSparseTerms(dtm, sparse)
  }
  else
  {
    dtms <- dtm
  }

  dtms
}

wf.generate <- function(dtm,
                        wc_freq,
                        wc_freq_scale,
                        seperator = FALSE)
{
  
  freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE) 
  wf <- data.frame(word=names(freq), freq=freq,stringsAsFactors = FALSE)   
  
  if(seperator)
  {
    wf$word <- gsub("^", "|", wf$word)
    wf$word <- gsub("$", "|", wf$word)
  }
  
  par(mfrow = c(1, 1))
  set.seed(142)   
  wordcloud(wf$word, freq, min.freq=2,max.words=wc_freq, 
            rot.per=0.15, scale=wc_freq_scale, 
            random.order=FALSE,
            colors=brewer.pal(6, "Dark2")) 

  wf
}


harmonicMean <- function(logLikelihoods, precision=2000L) {
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}

generate.lda <- function (dtm,topic_start_num = 50,topic_end_num =  100,
                          topic_num_interval = 5,max_iteration = 6,max_slope_angle=5,
                          burnin = 1000,iter = 1000,keep = 50)
{
  
  old_coeff <- 0
  for (i in 1:max_iteration)
  {
      sequ <- seq(topic_start_num, topic_end_num, topic_num_interval) # in this case a sequence of numbers from 10 to 50, by 2.
      
      set.seed(123)
      fitted_many <- lapply(sequ, function(k) LDA(dtm, 
                                                  k = k, 
                                                  method = "Gibbs",
                                                  control = list(burnin = burnin, 
                                                                 iter = iter, 
                                                                 keep = keep) ))
      
      # extract logliks from each topic
      logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
      
      # compute harmonic means
      hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
      
      # inspect
      plot(sequ, hm_many, type = "l")
      lmModel <- lm(hm_many ~ sequ)
      abline(lmModel,col="blue", lty=2, lwd=2)
      
      
      coeff <- (lmModel$coefficients[2]) /((max(hm_many)-min(hm_many))/(max(sequ)-min(sequ)))
      print(paste("  \n",Sys.time(),": Iteration ",i," - coefficient: ",coeff,
                  " - # topics:",sequ[which.max(hm_many)],"  \n"))
      
      change_interval <- 0
      if((sign(old_coeff) != 0) & (sign(coeff) != sign(old_coeff)))
      {
        if(topic_num_interval == 1) break 
        old_start <- topic_start_num
        old_end <- topic_end_num
        old_interval <- topic_num_interval
        change_interval <- 1
        
      }else if(coeff > tan(max_slope_angle * pi/180)) ## this means absolute slope angle is greater than a certain value in degree
      {
        old_start <- topic_start_num
        old_end <- topic_end_num
        if (sequ[which.max(hm_many)] != old_start)
        {
          topic_start_num <- sequ[(which.max(hm_many) - 1)]
        } else break
        
        if (topic_start_num < 2) topic_start_num <- 2
        topic_end_num <- topic_start_num + (old_end - old_start)
        if (old_start == topic_start_num & old_end == topic_end_num)
        {
          old_interval <- topic_num_interval
          change_interval <- 1
        }
        
      }else if (coeff < tan(-1 * max_slope_angle * pi/180)) ## this means absolute slope angle is less than a certain value in degree
      {
        old_start <- topic_start_num
        old_end <- topic_end_num
        if (sequ[which.max(hm_many)] != old_end)
        {
          topic_end_num <- sequ[(which.max(hm_many) + 1)]
        } else break
        topic_start_num <- topic_end_num - (old_end - old_start)
        if (topic_start_num < 2) topic_start_num <- 2
        
        if (old_start == topic_start_num & old_end == topic_end_num)
        {
          old_interval <- topic_num_interval
          change_interval <- 1
        }
      }
      else
      {
        if(topic_num_interval == 1) break 
        old_start <- topic_start_num
        old_end <- topic_end_num
        old_interval <- topic_num_interval
        change_interval <- 1
      }
      
      if(change_interval == 1)
      {
        topic_num_interval <- round(sqrt(topic_num_interval))
        topic_start_num <- sequ[which.max(hm_many)] - 
          round(((old_end - old_start)/(2 * old_interval))*topic_num_interval)
        topic_end_num <- sequ[which.max(hm_many)] + 
          round(((old_end - old_start)/(2 * old_interval))*topic_num_interval)
        if (topic_start_num < 2) topic_start_num <- 2
      }
      old_coeff <- coeff
      print(paste("  \nFor next iteration topic_num_interval: ",topic_num_interval,
                  ", topic_start_num: ",topic_start_num,
                  ", topic_end_num: ",topic_end_num,"  \n"))
  }
  
  # compute optimum number of topics
  #print(sequ[which.max(hm_many)])
  sub_title <- paste("The optimal number of topics is", 
                                                   sequ[which.max(hm_many)])
  
  ldaplot <- ggplot(data.frame(sequ, hm_many), aes(x=sequ, y=hm_many)) + 
    geom_path(lwd=1.5) +
                  theme(text = element_text(family= NULL),
        axis.title.y=element_text(vjust=1, size=16),
        axis.title.x=element_text(vjust=-.5, size=16),
        axis.text=element_text(size=16),
        plot.title=element_text(size=20)) +
        xlab('Number of Topics') +
        ylab('Harmonic Mean')  +
        ggtitle(bquote(atop("Latent Dirichlet Allocation Analysis of customer complaints", 
                          atop(italic(.(sub_title)), ""))))
      
  
  
  ldaModel <- fitted_many[[which.max(hm_many)]]
  
  list(lda_plot = ldaplot,lda_model = ldaModel)
  
}



generate.topic.terms <- function (ldaModel,num_topic_terms = 5)

{
  lda.topics <- as.data.frame(topics(ldaModel))
  names(lda.topics) <- "topic_id"
  lda.terms <- t(as.matrix(terms(ldaModel,num_topic_terms)))
  
  lda.terms.combined <- matrix(apply(as.data.frame(lda.terms),1,
                                     paste,collapse="|"))
  lda.topic.terms <- dplyr::mutate(lda.topics,
                            topics = lda.terms.combined[topic_id])  
  lda.topic.terms
}


remove.unclustered.dtm <- function(dtm)
{
  rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
  dtm.non_uncl <- dtm[rowTotals> 0,] 
  dtm.non_uncl
}

topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
        ## Find required quantities
     phi <- posterior(fitted)$terms %>% as.matrix
     theta <- posterior(fitted)$topics %>% as.matrix
     vocab <- colnames(phi)
     doc_length <- vector()
     for (i in 1:length(corpus)) {
          temp <- paste(corpus[[i]]$content, collapse = ' ')
          doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
     }
     temp_frequency <- as.matrix(doc_term)
     freq_matrix <- data.frame(ST = colnames(temp_frequency),
                               Freq = colSums(temp_frequency))
     rm(temp_frequency)

     ## Convert to json
     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
                                    vocab = vocab,
                                    doc.length = doc_length,
                                    term.frequency = freq_matrix$Freq)

     return(json_lda)
}

get.match.score <- function(all_string,words)
{
  score <- sum((str_count(all_string, words)) * 
                 (str_count(words,'\\w+') ^ str_count(words,'\\w+')))
  score
}

suggest_stop_words <- function(df,ng = 1:3,sparse=1)
{
  all_suggested_stop_words <- c()
  for (i in 1:length(ng))
  {
    content_DTM_4_stopwords <- dtm.generate(df,
                                ng[i],sparse,
                                keep.id=TRUE,
                                doIDF = TRUE,
                                doNormTf = TRUE)
    freq <- sort(colSums(as.matrix(content_DTM_4_stopwords)), decreasing=TRUE) 
    wf_4_stop_words <- data.frame(word=names(freq), freq=freq,stringsAsFactors = FALSE)
  
    
    #hist(log2(wf_4_stop_words$freq),breaks = 50)
    
    wf_4_stop_words$log_freq <- log2(wf_4_stop_words$freq)
    
    log_freq_iqr <- IQR(wf_4_stop_words$log_freq)
    log_freq_1stq <- as.numeric(quantile(wf_4_stop_words$log_freq,na.rm=T)[2])
    log_freq_med <- as.numeric(quantile(wf_4_stop_words$log_freq,na.rm=T)[3])
    log_freq_3rdq <- as.numeric(quantile(wf_4_stop_words$log_freq,na.rm=T)[4])
    
    suggested_stop_words <- wf_4_stop_words %>%
      filter(wf_4_stop_words$log_freq > log_freq_3rdq + 1.5 * log_freq_iqr |
      (wf_4_stop_words$log_freq >= log_freq_1stq - 1.5 * log_freq_iqr & 
         wf_4_stop_words$log_freq <= log_freq_med)) %>%
      select(word)
    
    all_suggested_stop_words <- c(all_suggested_stop_words,suggested_stop_words[,1])
  }
  suggested_stop_words_all <- as.data.frame(all_suggested_stop_words)
  names(suggested_stop_words_all) <- "word"
  suggested_stop_words_all$word <- as.character(suggested_stop_words_all$word)
  
  score <- c()
  for (i in 1:nrow(suggested_stop_words_all))
  { 
    score <- c(score,
               get.match.score(suggested_stop_words_all[-i,1],
                               suggested_stop_words_all[i,1]))
  }
  suggested_stop_words_all <- cbind(suggested_stop_words_all,score)
  suggested_stop_words_all <- suggested_stop_words_all[order(suggested_stop_words_all$score,
                                                             decreasing=TRUE),]
  recom_stop_words <- suggested_stop_words_all %>%
    filter(score > 0)
  
  return(recom_stop_words)
}

create_group_plots <- function(df,target,var_list,top_num = c(5,3))
{
  
  top_items <- as.data.frame(df %>%
            group_by_(.dots = var_list) %>%
            summarise(n = n()))

  top_items <- top_items[order(top_items$n,decreasing = T),][1:top_num[1],]
  
  reduced_dataset <- df
  for (i in 1:length(var_list))
  {
    reduced_dataset <- reduced_dataset[which(reduced_dataset[,var_list[i]] %in%
                                  top_items[,var_list[i]]),]
    #print(nrow(reduced_dataset))
  }

  hist(reduced_dataset[,target],
       breaks = 25,
       main=paste("Histogram for",target,"for top",top_num[1],var_list[i],sep =" "), 
        xlab=target)
  
  #reduced_dataset$req_year_mon <- as.yearmon(reduced_dataset[,date_field])
  
  if(length(var_list) == 1)
  {

    barDF_mean <- as.data.frame(df %>%
     filter_(paste("!is.na(",target,")")) %>%
     group_by_(.dots = c(var_list)) %>%
            summarise_(average = interp(~mean(var, na.rm = TRUE),
                                    var = as.name(target))))
    barDF_mean_dec <- barDF_mean[order(barDF_mean$average, decreasing = TRUE),]
    bar_plot_mean_dec <- ggplot(barDF_mean_dec[1:top_num[1],], 
                                aes_string(x = paste0("reorder(",
                                                as.name(colnames(barDF_mean_dec)[1]),
                                              ", -",as.name(colnames(barDF_mean_dec)[2]), 
                                                    ")"),
                                y = as.name(colnames(barDF_mean_dec)[2]))) +
              geom_bar(stat='identity') +
                theme(axis.text.x = element_text(angle = 90,hjust = 1)) +
        xlab(var_list[1]) +
        ylab(paste("Average ",target,sep=" ")) +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
     ggtitle(paste("Variation of average postitive",target,"for",var_list[1],sep=" "))
  
    print(bar_plot_mean_dec)
    
    barDF_mean_inc <- barDF_mean[order(barDF_mean$average, decreasing = FALSE),]
    bar_plot_mean_inc <- ggplot(barDF_mean_inc[1:top_num[1],], 
                                aes_string(x = paste0("reorder(",
                                                as.name(colnames(barDF_mean_inc)[1]),
                                              ", ",as.name(colnames(barDF_mean_inc)[2]), 
                                                    ")"),
                                y = as.name(colnames(barDF_mean_inc)[2]))) +
              geom_bar(stat='identity') +
                theme(axis.text.x = element_text(angle = 90,hjust = 1)) +
        xlab(var_list[1]) +
        ylab(paste("Average ",target,sep=" ")) +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
     ggtitle(paste("Variation of average negative",target,"for",var_list[1],sep=" "))
  
    print(bar_plot_mean_inc)
    
    barDF_max <- as.data.frame(df %>%
     filter_(paste("!is.na(",target,")")) %>%
     group_by_(.dots = c(var_list)) %>%
            summarise_(max_sent = interp(~max(var, na.rm = TRUE),
                                    var = as.name(target))))
    barDF_max <- barDF_max[order(barDF_max$max_sent, decreasing = TRUE),]
    bar_plot_max <- ggplot(barDF_max[1:top_num[1],], aes_string(x = paste0("reorder(",
                                                    as.name(colnames(barDF_max)[1]),
                                                  ", -",as.name(colnames(barDF_max)[2]), 
                                                    ")"),
                                y = as.name(colnames(barDF_max)[2]))) +
              geom_bar(stat='identity') +
                theme(axis.text.x = element_text(angle = 90,hjust = 1)) +
        xlab(var_list[1]) +
        ylab(paste("Most positive",target,sep=" ")) +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
     ggtitle(paste("Variation of most postitive",target,"for",var_list[1],sep=" "))
  
    print(bar_plot_max)
    
    barDF_min <- as.data.frame(df %>%
     filter_(paste("!is.na(",target,")")) %>%
     group_by_(.dots = c(var_list)) %>%
            summarise_(min_sent = interp(~min(var, na.rm = TRUE),
                                    var = as.name(target))))
    barDF_min <- barDF_min[order(barDF_min$min_sent, decreasing = FALSE),]
    bar_plot_min <- ggplot(barDF_min[1:top_num[1],], aes_string(x = paste0("reorder(",
                                                    as.name(colnames(barDF_min)[1]),
                                                  ",",as.name(colnames(barDF_min)[2]), 
                                                    ")"),
                                y = as.name(colnames(barDF_min)[2]))) +
              geom_bar(stat='identity') +
                theme(axis.text.x = element_text(angle = 90,hjust = 1)) +
        xlab(var_list[1]) +
        ylab(paste("Most negative",target,sep=" ")) +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
     ggtitle(paste("Variation of most negative",target,"for",var_list[1],sep=" "))
  
    print(bar_plot_min)
    
    group_box <- ggplot(aes_(y = as.name(target), x = as.name(var_list[1])), 
         data = reduced_dataset) + geom_boxplot()+
          theme(axis.text.x = element_text(angle = 90,hjust = 1))+
      scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
     ggtitle(paste("Variation of ",target,"for top",
                   top_num[1],var_list[1],sep=" "))
    
  } else {
    
    top_items2 <- as.data.frame(reduced_dataset %>%
      group_by_(var_list[2]) %>%
      summarise(n=n()))
    
    top_items2 <- top_items2[order(top_items2$n,decreasing = T),][1:top_num[2],]
    reduced_dataset2 <- reduced_dataset[which(reduced_dataset[,var_list[2]] %in%
                                  top_items2[,var_list[2]]),]
    
    
    group_box <- ggplot(aes_(y = as.name(target), 
                x = as.name(var_list[1]), 
                fill = as.name(var_list[2])), 
           data = reduced_dataset2) + geom_boxplot()+
      theme(axis.text.x = element_text(angle = 90,hjust = 1))+
      scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
     ggtitle(paste("Variation of",target,"for top",top_num[1],var_list[1],
                   "\nand top",top_num[2],var_list[2],"combination",sep=" "))
    

  }
  
  print(group_box)

}

genericSummary_revised <- function (text, k_min,
                                    k_max_pct = 0.1,
                            split = "\\.|!|\\?|\\\n{2,}", 
                            min = 5, 
                            breakdown = FALSE,...) 
{
    sentences <- unlist(strsplit(text, split = split, fixed = F))
    sentences <- gsub("\\\n"," ",sentences)
    if (breakdown == TRUE) {
        sentences <- breakdown(sentences)
    }
    k <- k_min
    if (ceiling(length(sentences) *  k_max_pct) > k_min)
    {
      k <- ceiling(length(sentences) *  k_max_pct)
    }
    sentences <- sentences[(stri_count(sentences,regex="\\S+") > min) &
                             (nchar(sentences) > 
                             (stri_count(sentences,regex="\\S+") * 2))]
    if(length(sentences) > k)
    {
      td = tempfile()
      dir.create(td)
      for (i in 1:length(sentences)) {
          docname <- paste("sentence", i, ".txt", sep = "")
          write(sentences[i], file = paste(td, docname, sep = "/"))
      }
      A <- textmatrix(td, ...)
      rownames <- rownames(A)
      colnames <- colnames(A)
      A <- matrix(A, nrow = nrow(A), ncol = ncol(A))
      rownames(A) <- rownames
      colnames(A) <- colnames
      unlink(td, T, T)
      Vt <- lsa(A, dims = length(sentences))$dk
      snum <- vector(length = k)
      for (i in 1:k) {
          snum[i] <- names(Vt[, i][abs(Vt[, i]) == max(abs(Vt[, 
              i]))])
      }
      snum <- gsub(snum, pattern = "[[:alpha:]]", replacement = "")
      snum <- gsub(snum, pattern = "[[:punct:]]", replacement = "")
      snum <- as.integer(snum)
      summary.sentences <- sentences[snum]
    }else{
      summary.sentences <- sentences 
    }
    summary.sentences
}
```


#Read input data

```{r read_data}

compDF <- read.csv("D:/Data Science/Automobile complaint analysis/input/auto_complaint.csv",
                      stringsAsFactors = F)

print(xtable(strtable(compDF)),
      type="html",include.rownames = FALSE)


```

# Data pre-processing

## Summarization for each complaint

```{r summ comp}

comp_sum <- c()
# for(i in 1:nrow(comp_topicDF))
# {
#     if(nsentence(comp_topicDF[i,"Complaint.Text"]) > 2)
#     {
#       comp_sum <- c(comp_sum,paste(genericSummary(comp_topicDF[i,"Complaint.Text"],2),
#                                                  collapse="  \n"))
#     } else {
#       comp_sum <- c(comp_sum,comp_topicDF[i,"Complaint.Text"])
#     }
# }

for(i in 1:nrow(compDF))
{
  #print(i)
  
  comp_sum <- c(comp_sum,paste(unique(genericSummary_revised(
                                      compDF[i,"Complaint.Text"],
                                                k_min=2,
                                                min=5)),
                                               collapse=".  \n"))
  
  # if(length(gregexpr('[[:alnum:] ]{5}[.!?]', 
  #                    compDF[i,"Complaint.Text"])[[1]]) > 2)
  # {
  #   comp_sum <- c(comp_sum,paste(genericSummary(gsub("[\\?|!]",".",
  #                                       compDF[i,"Complaint.Text"]),
  #                                               k=2,
  #                                               min=10),
  #                                              collapse="  \n"))
  # } else {
  #   comp_sum <- c(comp_sum,compDF[i,"Complaint.Text"])
  # }
}

compDF <- cbind.data.frame(compDF,complaint_summary = comp_sum,
                                 stringsAsFactors = FALSE)

```


### Interim file  

```{r write_interim_file, cache=FALSE}

interim_directory <- "D:/Data Science/Automobile complaint analysis/processed_data"
interim_file = paste(interim_directory,"/automobile_customer_complain_interim_",
                    format(Sys.Date(), "%Y%m%d"),".csv",sep="")

write.csv(compDF,interim_file,row.names=FALSE)
print(paste("  \nFollowing interim file created - ",interim_file,sep=""))

```


## POS tagging

Visualizing POS data for few complaints  

```{r sample_pos}

sample_pos <- tagPOS(compDF[1,"complaint_summary"])

sample_pos$POStagged

extractPOS(compDF[831,"complaint_summary"],c("RB","NN","JJ","VBN"))

extractPOS(compDF[545,"complaint_summary"],c("RB"))

```

We'll extract only following POS for further analysis  
RB -> Adverb  
NN -> Noun, singular or mass, including NNP -> Proper noun, singular (and plural)  
JJ -> Adjective  
VBN -> Verb, past participle  


```{r extract_pos}

comp_keywords <- lapply(compDF[,"complaint_summary"],
                        extractPOS,
                        POSregex_list = c("RB","NN","JJ","VBN")) 
                        #POSregex_exclude_list = c("NNP"))
comp_keywordsDF <- as.data.frame(do.call(rbind,comp_keywords))

comp_keywordsDF[,1] <- as.character(comp_keywordsDF[,1])

comp_keywordsDF <- cbind(compDF$S.No.,comp_keywordsDF,
                                  stringsAsFactors = FALSE)

names(comp_keywordsDF) <- c("ID","content")

```

## Stop words suggestion

```{r stop words suggestion}
recommended_stop_words <- suggest_stop_words(comp_keywordsDF,1:3,0.999)

# print(xtable(recommended_stop_words),
#       type="html",include.rownames = FALSE)

recom_stop_words_file <- "D:/Data Science/Automobile complaint analysis/processed_data/suggested_stop_words.csv"

write.csv(recommended_stop_words,recom_stop_words_file,row.names=FALSE)
print(paste("  \nFollowing output file created - ",recom_stop_words_file,sep=""))

```
  
These are only recommended stop words. Need to use single words stop words from the list  
  
## DTM creation

```{r dtm_1}

#special_symbols <- c("<p>","</p>","<blockquote>","</blockquote>")
stop_words_file <- "D:/Data Science/Automobile complaint analysis/input/stop_words.csv"

content_DTM_1 <- dtm.generate(comp_keywordsDF,
                              1,0.99,
                              #spl_sym = special_symbols,
                              my_stop_word_file = stop_words_file,
                              keep.id=TRUE,
                              doIDF = FALSE)

term1_tfidf <- tapply(content_DTM_1$v/slam::row_sums(content_DTM_1)[content_DTM_1$i], 
                     content_DTM_1$j, mean) *
              log2(tm::nDocs(content_DTM_1)/slam::col_sums(content_DTM_1 > 0))

summary(term1_tfidf)

term1_tfidf_iqr <- summary(term1_tfidf)[5] - summary(term1_tfidf)[2]

content_DTM_1.reduced <- content_DTM_1[,
                      term1_tfidf >= (summary(term1_tfidf)[2] - 1.5 * term1_tfidf_iqr)]
#         term1_tfidf <= (summary(term1_tfidf)[5] + 1.5 * term1_tfidf_iqr)]

# content_DTM_1.reduced <- content_DTM_1[,
#                   term1_tfidf >= (summary(term1_tfidf)[3])]

summary(slam::col_sums(content_DTM_1.reduced))

```

### Word cloud for single words

```{r wordclouds_1}

wf_content_DTM_1 <- wf.generate(dtm=content_DTM_1.reduced,
                                 wc_freq = 50,
                                 wc_freq_scale = c(3, .3))

```


```{r dtm_2}

content_DTM_2 <- dtm.generate(comp_keywordsDF,
                              2,0.999,
                              #spl_sym = special_symbols,
                              my_stop_word_file = stop_words_file,
                              keep.id=TRUE,
                              doIDF = FALSE)

term2_tfidf <- tapply(content_DTM_2$v/slam::row_sums(content_DTM_2)[content_DTM_2$i], 
                     content_DTM_2$j, mean) *
              log2(tm::nDocs(content_DTM_2)/slam::col_sums(content_DTM_2 > 0))

summary(term2_tfidf)

term2_tfidf_iqr <- summary(term2_tfidf)[5] - summary(term2_tfidf)[2]
content_DTM_2.reduced <- content_DTM_2[,
                        term2_tfidf >= (summary(term2_tfidf)[2] - 1.5 * term2_tfidf_iqr)]
# term2_tfidf <= (summary(term2_tfidf)[5] + 1.5 * term2_tfidf_iqr)]

# content_DTM_2.reduced <- content_DTM_2[,
#                       term2_tfidf >= (summary(term2_tfidf)[2])]
summary(slam::col_sums(content_DTM_2.reduced))
#content_DTM_2.reduced$v <- content_DTM_2.reduced$v * (2^2)
```

### Word cloud for two consecutive words  

```{r wordclouds_2}

wf_content_DTM_2 <- wf.generate(dtm=content_DTM_2.reduced,
                                 wc_freq = 50,
                                 wc_freq_scale = c(2, .2))


```


```{r dtm_3}

content_DTM_3 <- dtm.generate(comp_keywordsDF,
                              3,0.999,
                              #spl_sym = special_symbols,
                              my_stop_word_file = stop_words_file,
                              keep.id=TRUE,
                              doIDF = FALSE)

term3_tfidf <- tapply(content_DTM_3$v/slam::row_sums(content_DTM_3)[content_DTM_3$i], 
                     content_DTM_3$j, mean) *
              log2(tm::nDocs(content_DTM_3)/slam::col_sums(content_DTM_3 > 0))

summary(term3_tfidf)

term3_tfidf_iqr <- summary(term3_tfidf)[5] - summary(term3_tfidf)[2]

content_DTM_3.reduced <- content_DTM_3[,
                   term3_tfidf >= (summary(term3_tfidf)[2] - 1.5 * term3_tfidf_iqr)]
#       term3_tfidf <= (summary(term3_tfidf)[5] + 1.5 * term3_tfidf_iqr)]

# content_DTM_3.reduced <- content_DTM_3[,
#                         term3_tfidf >= (summary(term3_tfidf))[2]] 

summary(slam::col_sums(content_DTM_3.reduced))
#content_DTM_3.reduced$v <- content_DTM_3.reduced$v * (3^3)


```

### Word cloud for three consecutive words

```{r wordclouds_3}

wf_content_DTM_3 <- wf.generate(dtm=content_DTM_3.reduced,
                                wc_freq = 50,
                                wc_freq_scale = c(2, .2))


```


```{r dtm_consolidated}

content_DTM <- cbind(content_DTM_3.reduced,content_DTM_2.reduced)

content_DTM.uncl <- remove.unclustered.dtm(content_DTM)

```

### Word cloud for words that will be used for topic modelling

```{r wordclouds_consolidated}

wf_content_DTM.uncl <- wf.generate(dtm=content_DTM.uncl,
                                 wc_freq = 50,
                                 wc_freq_scale = c(2.5, .25),
                                 seperator = TRUE)


```


# Topic Modeling

```{r lda}

content.lda.list <- generate.lda(content_DTM.uncl)

content.lda.model <- content.lda.list$lda_model

content.lda.list$lda_plot
      
```

## Terms for 5 topics as an example
```{r inspect_lda}

complain.topics <- topicmodels::topics(content.lda.model, 1)
## In this case I am returning the top 30 terms.
complain.topics.terms <- as.data.frame(topicmodels::terms(content.lda.model, 30), 
                                       stringsAsFactors = FALSE)
print(xtable(complain.topics.terms[,1:5]),
      type="html",include.rownames = FALSE)


# Creates a dataframe to store the complaint Number and the most likely topic
doctopics.df <- as.data.frame(complain.topics)
doctopics.df <- dplyr::mutate(doctopics.df,
                              complain_id = rownames(doctopics.df))
colnames(doctopics.df)[1] <- "topic_id"
doctopics.df$complain_id <- as.integer(doctopics.df$complain_id)

## Adds topic number to original dataframe of lessons
comp_topicDF <- dplyr::inner_join(compDF, doctopics.df, 
                             by = c("S.No."="complain_id"))




```

## All topics with 5 top terms

```{r complain_topic_top_terms}

topicTerms <- tidyr::gather(complain.topics.terms, Topic)
topicTerms <- cbind(topicTerms, Rank = rep(1:30))
topTerms <- dplyr::filter(topicTerms, Rank < 6)
topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))
topTerms$Topic <- as.numeric(topTerms$Topic)
topicLabel <- data.frame()
for (i in 1:length(complain.topics.terms)){
     z <- dplyr::filter(topTerms, Topic == i)
     l <- as.data.frame(glue::collapse(z[,2], sep = "|" ), 
                        stringsAsFactors = FALSE)
     topicLabel <- rbind(topicLabel, l)

}

topicLabel <- cbind(topicLabel,rownames(topicLabel))
colnames(topicLabel) <- c("Label","topic_id")
topicLabel$topic_id <- as.numeric(as.character(topicLabel$topic_id))

comp_topicDF <- dplyr::inner_join(comp_topicDF,topicLabel,by="topic_id")

print(xtable(topicLabel),
      type="html",include.rownames = FALSE)

```

# Topic Visualization

## Topic Distances and term relevance
```{r topic visualisation, cache=FALSE}

comp_keywordsDF.reduced <- comp_keywordsDF[comp_keywordsDF$ID %in% 
                                             doctopics.df$complain_id,]

m <- list(id = "ID", content = colnames(comp_keywordsDF.reduced)[2])
myReader <- readTabular(mapping = m)
complain.corpus <- VCorpus(DataframeSource(comp_keywordsDF.reduced), 
                           readerControl = list(reader = myReader))

complain.json <- topicmodels_json_ldavis(content.lda.model, 
                                         complain.corpus, content_DTM.uncl)
serVis(complain.json,
       out.dir = 'vis2', 
       open.browser=FALSE)

```


<iframe width="1300" height="1000" src="vis2/index.html" frameborder="0"></iframe>


## Topic correlation

```{r word topic creation}

word.topics <- as.data.frame(t(topicmodels::posterior(content.lda.model)$term))
word <- rownames(word.topics)
word.topics <- cbind.data.frame(word,word.topics,
                                stringsAsFactors = F)

num.col <- ncol(word.topics)
corrplot(cor(word.topics[,2:num.col]), method = "circle")

topic_name <- c()
# create topic names using first five words
for (i in 2:num.col){
  top.words <- word.topics[order(-word.topics[,i]),1]
  topic_name <- c(topic_name,
                  paste(top.words[1:5], collapse = "|"))
}

# rename topics
colnames(word.topics) <- c("word",topic_name)

```

## Topic Clustering

```{r word clustering topics }

m  <- as.matrix(t(word.topics))
# # # m <- m[1:2, 1:3]
distMatrix <- dist(m[2:nrow(m),], method="euclidean")
#print(distMatrix)
#distMatrix <- dist(m, method="cosine")
#print(distMatrix)

groups <- hclust(distMatrix,method="ward.D")

par(mar = c(4,1,1,12))
dend <- as.dendrogram(groups)


#library(dendextend)
# Horizontal plot
dend %>% set("branches_k_color", k = 10) %>% plot(horiz = TRUE)
dend %>% rect.dendrogram(k = 10, horiz = TRUE, border = 8, lty = 5, lwd = 2)

abline(v = heights_per_k.dendrogram(dend)["10"], lwd = 2, lty = 2, col = "blue")

```

## Community detection
### Static networks

```{r topic correlation}
cor_threshold <- .2

cor_mat <- cor(word.topics[,2:num.col])
cor_mat[ cor_mat < cor_threshold ] <- 0
diag(cor_mat) <- 0
```


```{r static netwroks}

graph <- graph.adjacency(cor_mat, weighted=TRUE, mode="lower")

# E(graph)$edge.width <- E(graph)$weight
# V(graph)$label <- paste(1:(ncol(word.topics)-1))
# 
# 
# par(mar=c(0, 0, 3, 0))
# set.seed(110)
# plot.igraph(graph, edge.width = E(graph)$edge.width, 
#             edge.color = "blue", vertex.color = "white", vertex.size = 1,
#             vertex.frame.color = NA, vertex.label.color = "grey30")
# title("Strength Between Topics Based On Word Probabilities", cex.main=.8)

```
  
 
  

```{r community detection}
clp <- cluster_label_prop(graph)
class(clp)

plot(clp, graph, edge.width = E(graph)$edge.width, vertex.size = 2, vertex.label = "")
title("Community Detection in Topic Network", cex.main=.8)

V(graph)$community <- clp$membership
V(graph)$betweenness <- betweenness(graph, v = V(graph), directed = F)
V(graph)$degree <- degree(graph, v = V(graph))

```

  
No particular clusters found
  
### Dynamic visualization

```{r vis graph}
visIgraph(graph)

```
  


  
# Sentiment Analysis
```{r sentiments}

sentiment_syuzhet <- get_sentiment(comp_topicDF$complaint_summary)
sentiment_afinn <- get_sentiment(comp_topicDF$complaint_summary,method="afinn")
sentiment_bing <- get_sentiment(comp_topicDF$complaint_summary,method="bing")
sentiment_nrc <- get_sentiment(comp_topicDF$complaint_summary,method="nrc")

comp_topicDF <- cbind(comp_topicDF,
                      sentiment_syuzhet,
                      sentiment_afinn,
                      sentiment_bing,
                      sentiment_nrc)

comp_topicDF <- as.data.frame(comp_topicDF %>%
  rowwise() %>%
  mutate(sentiment_averaged = mean(c(sentiment_syuzhet,
                                     sentiment_afinn,
                                     sentiment_bing,
                                     sentiment_nrc), na.rm=T)))

```


## Visualizing Syuzhet sentiments

```{r syushet sentiment}

summary_sent_syuzhet <- summary(sentiment_syuzhet)

create_group_plots(comp_topicDF,
                   "sentiment_syuzhet",
                   "Label",
                   10)


```

## Visualizing Bing sentiments

```{r bing sentiment}

summary_sent_bing <- summary(sentiment_bing)

create_group_plots(comp_topicDF,
                   "sentiment_bing",
                   "Label",
                   10)

```

## Visualizing Afinn sentiments

```{r afinn sentiment}

summary_sent_afinn <- summary(sentiment_afinn)

create_group_plots(comp_topicDF,
                   "sentiment_afinn",
                   "Label",
                   10)

```


## Visualizing NRC sentiments

```{r nrc sentiment}

summary_sent_nrc <- summary(sentiment_nrc)

create_group_plots(comp_topicDF,
                   "sentiment_nrc",
                   "Label",
                   10)

```
  
## Visualizing Averaged out sentiments  
  
```{r averaged sentiment}

summary_sent_avg <- summary(comp_topicDF$sentiment_averaged)

create_group_plots(comp_topicDF,
                   "sentiment_averaged",
                   "Label",
                   10)

```


## Statistics of sentiment scores

```{r stat sentiment scores summary}

summary_sent <- rbind.data.frame(summary_sent_syuzhet,
                      summary_sent_bing,
                      summary_sent_afinn,
                      summary_sent_nrc,
                      summary_sent_avg)

names(summary_sent) <- names(summary(sentiment_syuzhet))
rownames(summary_sent) <- c("syuzhet","bing","afinn","nrc","average")

print(xtable(summary_sent),
      type="html",include.rownames = TRUE)

```

## Are sentiments scrores different from each other?

```{r sentiment scores diff}

score_test_df <- cbind.data.frame(score=sentiment_syuzhet,
                                  method="syuzhet",
                                  stringsAsFactors = FALSE)

score_test_df <- rbind(score_test_df,cbind.data.frame(score=sentiment_afinn,
                                                      method = "afinn",
                                  stringsAsFactors = FALSE))

score_test_df <- rbind(score_test_df,cbind.data.frame(score=sentiment_bing,
                                                      method="bing",
                                  stringsAsFactors = FALSE))

score_test_df <- rbind(score_test_df,cbind.data.frame(score=sentiment_nrc,
                                                      method="nrc",
                                  stringsAsFactors = FALSE))

score_test_df <- rbind(score_test_df,cbind.data.frame(
                                  score=comp_topicDF$sentiment_averaged,
                                                      method="average",
                                  stringsAsFactors = FALSE))

score_test_df$method <- as.factor(score_test_df$method)

a1 <- aov(score_test_df$score ~ score_test_df$method)

#plot(a1)

posthoc <- TukeyHSD(x=a1, 'score_test_df$method', conf.level=0.95)

posthoc_df <- as.data.frame(posthoc$`score_test_df$method`)


print(xtable(posthoc_df),
      type="html",include.rownames = TRUE)

```


## Sentiment Word cloud

### Data Preparation

```{r sentiment word cloud data preparation}

content_DTM.uncl_df <- as.data.frame.matrix(content_DTM.uncl)
content_DTM.uncl_df$id <- rownames(content_DTM.uncl_df) 
tmp <- comp_topicDF[,c("S.No.","sentiment_afinn")]
names(tmp) <- c("id","sent")
tmp$id <- as.character(tmp$id)

content_DTM.uncl_df <- inner_join(content_DTM.uncl_df,
                                  tmp,
                                  by=c("id"))
content_DTM.uncl_df <- cbind(sentiment=ifelse(content_DTM.uncl_df$sent > 0,
                                                 "positive",
                                                 ifelse(content_DTM.uncl_df$sent < 0,
                                                        "negative","neutral")),
                              content_DTM.uncl_df)

sent_levels <- levels(factor(content_DTM.uncl_df$sentiment))
labels <- lapply(sent_levels, 
                 function(x) paste(x,format(round((length((
                content_DTM.uncl_df[content_DTM.uncl_df$sentiment ==x,])$id)/
                     length(content_DTM.uncl_df$sentiment)*100),2),nsmall=2),"%"))

sentiment_words_df <- as.data.frame(content_DTM.uncl_df %>%
                      select(-id,-sent) %>%
                      group_by(sentiment) %>%
                      summarise_all(sum))




rownames(sentiment_words_df)  <- labels
sentiment_words_df <- sentiment_words_df %>%
                      select(-sentiment)

word_sentiment_mat <- t(as.matrix(sentiment_words_df))


```

### Sentiment Word cloud for bi and tri grams

```{r sentiment word cloud}


# comparison word cloud
comparison.cloud(word_sentiment_mat, 
                  max.words=100,
                 rot.per = 0.15,
                 colors = c("red","blue","green"),
                  scale = c(2.5,.25), 
                 random.order = FALSE, 
                 title.size = 1.5)

```

# Topic summarization

## Summarization for each Topic using LexRank
```{r summ}

topic_summary_DF <- data.frame()
for(top_id in unique(comp_topicDF$topic_id))
{
  temp_df4sum <- comp_topicDF %>%
    filter(topic_id == top_id)
  temp_sum_df <- lexRank(temp_df4sum[,"complaint_summary"],
                         threshold = 0.18,
                         n=ceiling(nrow(temp_df4sum) * 0.1),
                         Verbose = FALSE)
  
  topic_summary <- c(topic_id=top_id,
                     topic_summary_txt = paste(unique(temp_sum_df[,"sentence"]),
                                               collapse=".  \n"))
  topic_summary_DF <- rbind(topic_summary_DF,topic_summary,
                            stringsAsFactors =FALSE)
  names(topic_summary_DF) <- c("topic_id","topic_summary_txt")
}

topic_summary_DF$topic_id <- as.numeric(topic_summary_DF$topic_id) 
topic_summary_DF <- inner_join(topicLabel,topic_summary_DF,
                               by = "topic_id")

print(xtable(topic_summary_DF),
      type="html",include.rownames = FALSE)

```
  
## Summarization for each Topic using LSA

```{r summ lsa}

topic_lsa_summary_DF <- data.frame()

for(top_id in unique(comp_topicDF$topic_id))
{
  temp_df4sum <- comp_topicDF %>%
    filter(topic_id == top_id)
  
  topic_lsa_summary_txt <- paste(unique(genericSummary_revised(paste(
                                      temp_df4sum[,"complaint_summary"],collapse=".  \n"),
                                                k_min=ceiling(nrow(temp_df4sum) * 0.1),
                                                min=5)),
                                               collapse=".  \n")

  topic_lsa_summary <- c(topic_id=top_id,
                     topic_summary_txt = topic_lsa_summary_txt)
  
  topic_lsa_summary_DF <- rbind(topic_lsa_summary_DF,topic_lsa_summary,
                            stringsAsFactors =FALSE)
  names(topic_lsa_summary_DF) <- c("topic_id","topic_summary_txt")
}

topic_lsa_summary_DF$topic_id <- as.numeric(topic_lsa_summary_DF$topic_id) 
topic_lsa_summary_DF <- inner_join(topicLabel,topic_lsa_summary_DF,
                               by = "topic_id")

print(xtable(topic_lsa_summary_DF),
      type="html",include.rownames = FALSE)

```


# Output file  

```{r write_final_output_xlsx, cache=FALSE}

output_directory <- "D:/Data Science/Automobile complaint analysis/processed_data"
output_file = paste(output_directory,"/automobile_customer_complain_topics_",
                    format(Sys.Date(), "%Y%m%d"),".csv",sep="")

write.csv(comp_topicDF,output_file,row.names=FALSE)
print(paste("  \nFollowing output file created - ",output_file,sep=""))

```